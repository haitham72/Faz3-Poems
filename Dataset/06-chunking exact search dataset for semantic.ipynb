{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af3b772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "\n",
    "def parse_entity_field(field_str):\n",
    "    if pd.isna(field_str) or not isinstance(field_str, str) or not field_str.strip():\n",
    "        return []\n",
    "    clean = re.sub(r'^شخص\\s*\\n?', '', field_str.strip())\n",
    "    if not clean:\n",
    "        return []\n",
    "\n",
    "    segments = re.split(r'\\]\\s*,\\s*\\[', clean)\n",
    "    records = []\n",
    "    for seg in segments:\n",
    "        seg = seg.strip('[] \\n')\n",
    "        if not seg:\n",
    "            continue\n",
    "        name = re.search(r'name:([^,]+)', seg)\n",
    "        rel = re.search(r'relation:([^,]+)', seg)\n",
    "        res = re.search(r'resolved_from:(.+)', seg)\n",
    "        if name and rel and res:\n",
    "            resolved_list = [x.strip() for x in res.group(1).split(',') if x.strip()]\n",
    "            records.append({\n",
    "                \"name\": name.group(1).strip(),\n",
    "                \"relation\": rel.group(1).strip(),\n",
    "                \"resolved_from\": resolved_list\n",
    "            })\n",
    "    return records\n",
    "\n",
    "def parse_places_field(field_str):\n",
    "    if pd.isna(field_str) or not isinstance(field_str, str) or not field_str.strip():\n",
    "        return []\n",
    "    \n",
    "    parts = re.split(r'name:', field_str)\n",
    "    records = []\n",
    "    for part in parts:\n",
    "        if not part.strip():\n",
    "            continue\n",
    "        name_type = part.split('type:', 1)\n",
    "        if len(name_type) == 2:\n",
    "            name = name_type[0].strip()\n",
    "            type_val = name_type[1].strip()\n",
    "            records.append({\n",
    "                \"name\": name,\n",
    "                \"type\": type_val\n",
    "            })\n",
    "    return records\n",
    "\n",
    "def parse_topics_field(field_str):\n",
    "    if pd.isna(field_str) or not isinstance(field_str, str) or not field_str.strip():\n",
    "        return []\n",
    "    topics = [topic.strip() for topic in field_str.split(',') if topic.strip()]\n",
    "    return topics  # Returns list for JSON serialization\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"06 - FAZ3_POEMS_Exact_Search - Exact_search.csv\")\n",
    "\n",
    "# Parse all three columns\n",
    "df['parsed_entities'] = df['شخص'].apply(lambda x: json.dumps(parse_entity_field(x), ensure_ascii=False))\n",
    "df['parsed_places'] = df['أماكن'].apply(lambda x: json.dumps(parse_places_field(x), ensure_ascii=False))\n",
    "df['parsed_topics'] = df['مواضيع'].apply(lambda x: json.dumps(parse_topics_field(x), ensure_ascii=False))\n",
    "\n",
    "# Replace nulls in new columns with '[]'\n",
    "df['parsed_entities'] = df['parsed_entities'].apply(lambda x: '[]' if pd.isna(x) else x)\n",
    "df['parsed_places'] = df['parsed_places'].apply(lambda x: '[]' if pd.isna(x) else x)\n",
    "df['parsed_topics'] = df['parsed_topics'].apply(lambda x: '[]' if pd.isna(x) else x)\n",
    "\n",
    "# Fill all object columns with '[]' for nulls\n",
    "object_cols = df.select_dtypes(include=['object']).columns\n",
    "df[object_cols] = df[object_cols].fillna('[]')\n",
    "\n",
    "# Save\n",
    "df.to_csv(\"06 - Exact_search.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13d1e436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 1417 chunks from 4213 original lines\n",
      "Output saved to: 07-chunked for semantic.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from ast import literal_eval\n",
    "\n",
    "def chunk_semantic_search_dataset(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Chunk semantic search dataset with:\n",
    "    - 4 lines per chunk\n",
    "    - 1-line overlap between chunks (step=3)\n",
    "    - Final chunk always contains last 4 lines\n",
    "    - Strict poem boundaries (no cross-poem chunks)\n",
    "    - Preserves all original columns, aggregating per-line columns without duplicates using set-like logic\n",
    "    - Handles short poems appropriately\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_csv, encoding='utf-8-sig')\n",
    "    df = df.sort_values(['poem_id', 'Row_ID']).reset_index(drop=True)\n",
    "    \n",
    "    # Identify poem-level columns (assuming same value across poem)\n",
    "    poem_level_cols = ['Title_raw', 'Title_cleaned', 'قافية', 'روي', 'البحر', 'وصل', 'حركة', 'شخص', 'تصنيف', 'مواضيع']\n",
    "    \n",
    "    # Per-line aggregatable list columns (string reps of lists or JSON)\n",
    "    list_cols = ['sentiments', 'parsed_places', 'parsed_topics', 'دين', 'أحداث', 'أماكن', 'parsed_entities']\n",
    "    \n",
    "    output_rows = []\n",
    "    chunk_id_counter = 1\n",
    "    \n",
    "    for poem_id, group in df.groupby('poem_id'):\n",
    "        lines = group['Poem_line_cleaned'].tolist()\n",
    "        raw_lines = group['Poem_line_raw'].tolist()\n",
    "        summaries = group['summary'].tolist()\n",
    "        row_ids = group['Row_ID'].tolist()\n",
    "        full_poem = '\\n'.join(lines)\n",
    "        \n",
    "        # Poem-level values\n",
    "        poem_values = {col: group[col].iloc[0] if col in group.columns else '' for col in poem_level_cols}\n",
    "        \n",
    "        n_lines = len(lines)\n",
    "        \n",
    "        if n_lines < 4:\n",
    "            start_indices = [0]\n",
    "        else:\n",
    "            regular_starts = list(range(0, n_lines - 3, 3))\n",
    "            final_start = n_lines - 4\n",
    "            start_indices = sorted(set(regular_starts + [final_start]))\n",
    "        \n",
    "        for start_idx in start_indices:\n",
    "            end_idx = start_idx + min(4, n_lines - start_idx)\n",
    "            chunk_group = group.iloc[start_idx:end_idx]\n",
    "            \n",
    "            # Aggregations\n",
    "            chunk_text = '\\n'.join(lines[start_idx:end_idx])\n",
    "            chunk_raw_text = '\\n'.join(raw_lines[start_idx:end_idx])\n",
    "            chunk_summaries = summaries[start_idx:end_idx]\n",
    "            summary_chunked = ' '.join(chunk_summaries) + \"---\" + chunk_text\n",
    "            row_id_str = ','.join(map(str, row_ids[start_idx:end_idx]))\n",
    "            chunk_type = 'short_poem' if n_lines < 4 else ('final' if start_idx == n_lines - 4 else 'regular')\n",
    "            \n",
    "            # Aggregate list columns without duplicates\n",
    "            agg_lists = {}\n",
    "            for col in list_cols:\n",
    "                if col in chunk_group.columns:\n",
    "                    unique_items = set()\n",
    "                    for val in chunk_group[col].dropna():\n",
    "                        try:\n",
    "                            items = json.loads(val)\n",
    "                            for item in items:\n",
    "                                if isinstance(item, dict):\n",
    "                                    unique_items.add(json.dumps(item, sort_keys=True, ensure_ascii=False))\n",
    "                                else:\n",
    "                                    unique_items.add(str(item))\n",
    "                        except:\n",
    "                            # If not JSON, treat as string and try to parse as list if possible\n",
    "                            if val.strip().startswith('[') and val.strip().endswith(']'):\n",
    "                                try:\n",
    "                                    items = literal_eval(val)\n",
    "                                    for item in items:\n",
    "                                        if isinstance(item, dict):\n",
    "                                            unique_items.add(json.dumps(item, sort_keys=True, ensure_ascii=False))\n",
    "                                        else:\n",
    "                                            unique_items.add(str(item))\n",
    "                                except:\n",
    "                                    unique_items.add(str(val))\n",
    "                            else:\n",
    "                                unique_items.add(str(val))\n",
    "                    \n",
    "                    # Reconstruct as list string with readable Arabic\n",
    "                    reconstructed = []\n",
    "                    for s in sorted(unique_items):\n",
    "                        try:\n",
    "                            reconstructed.append(json.loads(s))\n",
    "                        except:\n",
    "                            reconstructed.append(s)\n",
    "                    \n",
    "                    # Store as JSON string for consistency\n",
    "                    agg_lists[col] = json.dumps(reconstructed, ensure_ascii=False)\n",
    "            \n",
    "            # Build chunk row\n",
    "            chunk_row = {\n",
    "                'poem_id': poem_id,\n",
    "                'chunk_id': chunk_id_counter,\n",
    "                'Title_cleaned': poem_values['Title_cleaned'],\n",
    "                'poem_lines_subset': chunk_text,\n",
    "                'Summary_chunked': summary_chunked,\n",
    "                'Title_raw': poem_values['Title_raw'],\n",
    "                'Poem_line_raw': chunk_raw_text,\n",
    "                'Full_poem': full_poem,\n",
    "                'Row_IDs_in_chunk': row_id_str,\n",
    "                'chunk_type': chunk_type,\n",
    "                **poem_values,  # Include all poem-level\n",
    "                **agg_lists,    # Include aggregated lists\n",
    "            }\n",
    "            \n",
    "            # Add ALL remaining original columns (not in handled lists) - take first value for consistency\n",
    "            all_original_cols = df.columns.tolist()\n",
    "            handled_cols = ['poem_id', 'Row_ID', 'Poem_line_raw', 'Poem_line_cleaned', 'summary'] + poem_level_cols + list_cols\n",
    "            for col in all_original_cols:\n",
    "                if col not in handled_cols:\n",
    "                    chunk_row[col] = chunk_group[col].iloc[0] if col in chunk_group.columns and len(chunk_group[col].dropna()) > 0 else ''\n",
    "            \n",
    "            output_rows.append(chunk_row)\n",
    "            chunk_id_counter += 1\n",
    "    \n",
    "    output_df = pd.DataFrame(output_rows)\n",
    "    output_df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Successfully created {len(output_df)} chunks from {len(df)} original lines\")\n",
    "    print(f\"Output saved to: {output_csv}\")\n",
    "    return output_df\n",
    "\n",
    "# Direct execution\n",
    "input_file = \"06 - Exact_search.csv\"  # Use your processed CSV file\n",
    "output_file = \"07-chunked for semantic.csv\"\n",
    "\n",
    "result = chunk_semantic_search_dataset(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4e13315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1417 rows, merged person entities in 'شخص' column\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def merge_person_entities(escaped_json_str):\n",
    "    \"\"\"Merge person entities by name/relation, combining resolved_from lists\"\"\"\n",
    "    if pd.isna(escaped_json_str) or escaped_json_str == '':\n",
    "        return escaped_json_str\n",
    "    \n",
    "    try:\n",
    "        # Handle escaped JSON string by unescaping quotes first\n",
    "        if escaped_json_str.startswith('\"') and escaped_json_str.endswith('\"'):\n",
    "            # Remove outer quotes and unescape inner quotes\n",
    "            json_str = escaped_json_str[1:-1].replace('\\\\\"', '\"')\n",
    "        else:\n",
    "            json_str = escaped_json_str\n",
    "            \n",
    "        items = json.loads(json_str)\n",
    "        merged_dict = {}\n",
    "        \n",
    "        for item in items:\n",
    "            name = item['name']\n",
    "            relation = item['relation']\n",
    "            resolved_from = item['resolved_from']\n",
    "            \n",
    "            key = (name, relation)\n",
    "            if key in merged_dict:\n",
    "                # Combine resolved_from lists, avoiding duplicates while preserving order\n",
    "                existing_resolved = merged_dict[key]\n",
    "                for res in resolved_from:\n",
    "                    if res not in existing_resolved:\n",
    "                        existing_resolved.append(res)\n",
    "            else:\n",
    "                merged_dict[key] = resolved_from[:]\n",
    "        \n",
    "        # Create the merged list\n",
    "        merged_list = []\n",
    "        for (name, relation), resolved_from in merged_dict.items():\n",
    "            merged_list.append({\n",
    "                'name': name,\n",
    "                'relation': relation,\n",
    "                'resolved_from': resolved_from\n",
    "            })\n",
    "        \n",
    "        return json.dumps(merged_list, ensure_ascii=False)\n",
    "    except:\n",
    "        return escaped_json_str\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(\"07-chunked for semantic.csv\", encoding='utf-8-sig')\n",
    "\n",
    "# Apply merging to the 'شخص' column\n",
    "if 'شخص' in df.columns:\n",
    "    df['شخص'] = df['شخص'].apply(merge_person_entities)\n",
    "\n",
    "# Save the updated CSV\n",
    "df.to_csv(\"07- 02 -chunked for semantic.csv\", index=False, encoding='utf-8-sig')\n",
    "print(f\"Processed {len(df)} rows, merged person entities in 'شخص' column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f0359e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
