{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13871d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 1418 chunks from 4215 original lines\n",
      "Output saved to: 07-chunked for semantic.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def chunk_semantic_search_dataset(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Chunk semantic search dataset with:\n",
    "    - 4 lines per chunk\n",
    "    - 1-line overlap between chunks (step=3)\n",
    "    - Final chunk always contains last 4 lines\n",
    "    - Strict poem boundaries (no cross-poem chunks)\n",
    "    - Preserves all original columns including Arabic metadata\n",
    "    - Handles short poems appropriately\n",
    "    \"\"\"\n",
    "    # Read input CSV\n",
    "    df = pd.read_csv(input_csv, encoding='utf-8-sig')\n",
    "    \n",
    "    # Sort by poem_id and Row_ID to ensure correct ordering\n",
    "    df = df.sort_values(['poem_id', 'Row_ID']).reset_index(drop=True)\n",
    "    \n",
    "    output_rows = []\n",
    "    chunk_id_counter = 1\n",
    "    \n",
    "    # Process each poem separately\n",
    "    for poem_id, group in df.groupby('poem_id'):\n",
    "        lines = group['Poem_line_cleaned'].tolist()\n",
    "        raw_lines = group['Poem_line_raw'].tolist()\n",
    "        row_ids = group['Row_ID'].tolist()\n",
    "        title_raw = group['Title_raw'].iloc[0]  # Same for entire poem\n",
    "        title_cleaned = group['Title_cleaned'].iloc[0]  # Same for entire poem\n",
    "        \n",
    "        # Get first value for summary and other repeated columns\n",
    "        summary = group['summary'].iloc[0] if 'summary' in group.columns else ''\n",
    "        \n",
    "        # Store other Arabic metadata columns\n",
    "        qafia = group['قافية'].iloc[0] if 'قافية' in group.columns else ''\n",
    "        al_bahr = group['البحر'].iloc[0] if 'البحر' in group.columns else ''\n",
    "        wasl = group['وصل'].iloc[0] if 'وصل' in group.columns else ''\n",
    "        haraka = group['حركة'].iloc[0] if 'حركة' in group.columns else ''\n",
    "        noa3 = group['نوع'].iloc[0] if 'نوع' in group.columns else ''\n",
    "        shakhs = group['شخص'].iloc[0] if 'شخص' in group.columns else ''\n",
    "        sentiments = group['sentiments'].iloc[0] if 'sentiments' in group.columns else ''\n",
    "        amakin = group['أماكن'].iloc[0] if 'أماكن' in group.columns else ''\n",
    "        a7dath = group['أحداث'].iloc[0] if 'أحداث' in group.columns else ''\n",
    "        mawadi3 = group['مواضيع'].iloc[0] if 'مواضيع' in group.columns else ''\n",
    "        tasnif = group['تصنيف'].iloc[0] if 'تصنيف' in group.columns else ''\n",
    "        \n",
    "        # Get all other original columns (excluding the ones we've already handled)\n",
    "        other_cols = {col: group[col].iloc[0] for col in df.columns \n",
    "                     if col not in ['Row_ID', 'Poem_line_raw', 'Poem_line_cleaned', \n",
    "                                   'Title_raw', 'Title_cleaned', 'summary', 'قافية', \n",
    "                                   'البحر', 'وصل', 'حركة', 'نوع', 'شخص', 'sentiments', \n",
    "                                   'أماكن', 'أحداث', 'مواضيع', 'تصنيف']}\n",
    "        \n",
    "        n_lines = len(lines)\n",
    "        \n",
    "        # Case 1: Poem has fewer than 4 lines - create single chunk\n",
    "        if n_lines < 4:\n",
    "            chunk_text = '\\n'.join(lines)\n",
    "            chunk_raw_text = '\\n'.join(raw_lines)\n",
    "            row_id_str = ','.join(map(str, row_ids))\n",
    "            \n",
    "            chunk_row = {\n",
    "                'poem_id': poem_id,\n",
    "                'chunk_id': chunk_id_counter,\n",
    "                'Title_raw': title_raw,\n",
    "                'Title_cleaned': title_cleaned,\n",
    "                'Poem_line_raw': chunk_raw_text,\n",
    "                'Poem_line_cleaned': chunk_text,\n",
    "                'Row_IDs_in_chunk': row_id_str,\n",
    "                'chunk_type': 'short_poem',\n",
    "                'summary': summary,\n",
    "                'قافية': qafia,\n",
    "                'البحر': al_bahr,\n",
    "                'وصل': wasl,\n",
    "                'حركة': haraka,\n",
    "                'نوع': noa3,\n",
    "                'شخص': shakhs,\n",
    "                'sentiments': sentiments,\n",
    "                'أماكن': amakin,\n",
    "                'أحداث': a7dath,\n",
    "                'مواضيع': mawadi3,\n",
    "                'تصنيف': tasnif,\n",
    "                **other_cols  # Include all other original columns\n",
    "            }\n",
    "            output_rows.append(chunk_row)\n",
    "            chunk_id_counter += 1\n",
    "            continue\n",
    "        \n",
    "        # Case 2: Poem has 4+ lines\n",
    "        regular_starts = []\n",
    "        current_start = 0\n",
    "        \n",
    "        # Generate regular chunks with step=3 (1-line overlap)\n",
    "        while current_start <= n_lines - 4:\n",
    "            regular_starts.append(current_start)\n",
    "            current_start += 3\n",
    "        \n",
    "        # Determine final chunk start position (must end at last line)\n",
    "        final_start = n_lines - 4  # Always starts at last_line-3\n",
    "        \n",
    "        # Combine regular starts with final chunk start if needed\n",
    "        all_starts = sorted(set(regular_starts + [final_start]))\n",
    "        \n",
    "        # Create chunks\n",
    "        for start_idx in all_starts:\n",
    "            end_idx = start_idx + 4\n",
    "            # Safety check - should always have 4 lines here\n",
    "            if end_idx > n_lines:\n",
    "                continue\n",
    "                \n",
    "            chunk_lines = lines[start_idx:end_idx]\n",
    "            chunk_raw_lines = raw_lines[start_idx:end_idx]\n",
    "            chunk_row_ids = row_ids[start_idx:end_idx]\n",
    "            \n",
    "            chunk_text = '\\n'.join(chunk_lines)\n",
    "            chunk_raw_text = '\\n'.join(chunk_raw_lines)\n",
    "            row_id_str = ','.join(map(str, chunk_row_ids))\n",
    "            chunk_type = 'final' if start_idx == final_start and start_idx not in regular_starts else 'regular'\n",
    "            \n",
    "            chunk_row = {\n",
    "                'poem_id': poem_id,\n",
    "                'chunk_id': chunk_id_counter,\n",
    "                'Title_raw': title_raw,\n",
    "                'Title_cleaned': title_cleaned,\n",
    "                'Poem_line_raw': chunk_raw_text,\n",
    "                'Poem_line_cleaned': chunk_text,\n",
    "                'Row_IDs_in_chunk': row_id_str,\n",
    "                'chunk_type': chunk_type,\n",
    "                'summary': summary,\n",
    "                'قافية': qafia,\n",
    "                'البحر': al_bahr,\n",
    "                'وصل': wasl,\n",
    "                'حركة': haraka,\n",
    "                'نوع': noa3,\n",
    "                'شخص': shakhs,\n",
    "                'sentiments': sentiments,\n",
    "                'أماكن': amakin,\n",
    "                'أحداث': a7dath,\n",
    "                'مواضيع': mawadi3,\n",
    "                'تصنيف': tasnif,\n",
    "                **other_cols  # Include all other original columns\n",
    "            }\n",
    "            output_rows.append(chunk_row)\n",
    "            chunk_id_counter += 1\n",
    "    \n",
    "    # Create output DataFrame\n",
    "    output_df = pd.DataFrame(output_rows)\n",
    "    \n",
    "    # Save to CSV with UTF-8 encoding for Arabic text\n",
    "    output_df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Successfully created {len(output_df)} chunks from {len(df)} original lines\")\n",
    "    print(f\"Output saved to: {output_csv}\")\n",
    "    return output_df\n",
    "\n",
    "# Direct execution\n",
    "input_file = \"06-chunking exact search dataset for semantic.csv\"\n",
    "output_file = \"07-chunked for semantic.csv\"\n",
    "\n",
    "result = chunk_semantic_search_dataset(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1e436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
