{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13871d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! 2170 rows generated.\n",
      "Columns in output: ['poem_id', 'chunk_id', 'Title_cleaned', 'poem_lines_subset', 'Full_poem', 'Poem_line_raw', 'Title_raw', 'summary', 'قافية', 'البحر', 'وصل', 'حركة', 'نوع', 'شخص', 'sentiments', 'أماكن', 'أحداث', 'مواضيع', 'تصنيف']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import ast\n",
    "import json\n",
    "\n",
    "def merge_metadata_values(series_list, col_name):\n",
    "    \"\"\"Merge values from multiple rows, deduplicating appropriately\"\"\"\n",
    "    # Only exclude the source column that we're chunking from\n",
    "    if col_name in ['Poem_line_cleaned']:\n",
    "        return series_list.iloc[0] if not series_list.empty else \"\"\n",
    "    \n",
    "    # For JSON-like columns (person, sentiments, etc.)\n",
    "    if col_name in ['person', 'sentiments', 'أماكن', 'أحداث', 'مواضيع']:\n",
    "        all_values = []\n",
    "        for val in series_list:\n",
    "            if pd.notna(val) and val != \"\":\n",
    "                try:\n",
    "                    # Try parsing as JSON list/array\n",
    "                    parsed = ast.literal_eval(str(val)) if isinstance(val, str) and (val.startswith('[') or val.startswith('{')) else val\n",
    "                    if isinstance(parsed, list):\n",
    "                        all_values.extend(parsed)\n",
    "                    else:\n",
    "                        all_values.append(val)\n",
    "                except:\n",
    "                    all_values.append(val)\n",
    "        # Remove duplicates while preserving order\n",
    "        unique_values = []\n",
    "        seen = set()\n",
    "        for item in all_values:\n",
    "            item_str = json.dumps(item, sort_keys=True) if isinstance(item, dict) else str(item)\n",
    "            if item_str not in seen:\n",
    "                seen.add(item_str)\n",
    "                unique_values.append(item)\n",
    "        return unique_values\n",
    "    \n",
    "    # For simple text columns (summary, qafia, etc.), get unique values\n",
    "    else:\n",
    "        unique_values = list(set([str(v) for v in series_list if pd.notna(v) and v != \"\"]))\n",
    "        # Return the most common non-empty value, or join if multiple\n",
    "        if len(unique_values) == 1:\n",
    "            return unique_values[0]\n",
    "        else:\n",
    "            # For columns like summary, take the first one; for others, join unique values\n",
    "            if col_name == 'summary':\n",
    "                return series_list.iloc[0] if not series_list.empty else \"\"\n",
    "            else:\n",
    "                return \", \".join([v for v in unique_values if v])\n",
    "\n",
    "df = pd.read_csv(\"06-chunking exact search dataset for semantic.csv\")  # Replace with your actual filename\n",
    "\n",
    "required = ['poem_id', 'Row_ID', 'Title_cleaned', 'Poem_line_cleaned']\n",
    "for col in required:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Missing column: {col}\")\n",
    "\n",
    "df = df.sort_values(['poem_id', 'Row_ID']).reset_index(drop=True)\n",
    "df['Poem_line_cleaned'] = df['Poem_line_cleaned'].fillna('').astype(str)\n",
    "\n",
    "all_records = []\n",
    "chunk_counter = 1\n",
    "\n",
    "for poem_id, group in df.groupby('poem_id'):\n",
    "    title = group['Title_cleaned'].dropna().iloc[0] if not group['Title_cleaned'].dropna().empty else \"Untitled\"\n",
    "    poem_lines = [(row['Row_ID'], row['Poem_line_cleaned'], row) for _, row in group.iterrows()]\n",
    "    n = len(poem_lines)\n",
    "    \n",
    "    if n == 0:\n",
    "        # Handle empty poem case\n",
    "        base_record = {\n",
    "            'poem_id': poem_id,\n",
    "            'chunk_id': chunk_counter,\n",
    "            'Title_cleaned': title,\n",
    "            'poem_lines_subset': \"\",\n",
    "            'Poem_line_raw': \"\",\n",
    "            'Title_raw': \"\"\n",
    "        }\n",
    "        chunk_counter += 1\n",
    "        # Add all other columns with empty values\n",
    "        for col in group.columns:\n",
    "            if col not in ['poem_id', 'Row_ID', 'Title_cleaned', 'Poem_line_cleaned']:\n",
    "                base_record[col] = \"\"\n",
    "        all_records.append(base_record)\n",
    "    else:\n",
    "        full_raw = '\\n'.join(row['Poem_line_cleaned'] for _, _, row in poem_lines)\n",
    "        \n",
    "        # Format chunk content as clean lines without any tags\n",
    "        def format_chunk_content(lines_chunk):\n",
    "            if not lines_chunk:\n",
    "                return \"\"\n",
    "            # Join lines with newlines, no tags or iterators\n",
    "            return '\\n'.join([line for _, line, _ in lines_chunk])\n",
    "\n",
    "        # For poems with <= 4 lines, create just ONE chunk\n",
    "        if n <= 4:\n",
    "            chunk = poem_lines\n",
    "            base_record = {\n",
    "                'poem_id': poem_id,\n",
    "                'chunk_id': chunk_counter,\n",
    "                'Title_cleaned': title,\n",
    "                'poem_lines_subset': format_chunk_content(chunk),\n",
    "                'Poem_line_raw': \"\\n\".join([row['Poem_line_raw'] for _, row in group.iterrows()]) if 'Poem_line_raw' in group.columns else \"\",\n",
    "                'Title_raw': merge_metadata_values(group['Title_raw'], 'Title_raw') if 'Title_raw' in group.columns else \"\"\n",
    "            }\n",
    "            \n",
    "            # Add ALL original columns\n",
    "            for col in group.columns:\n",
    "                if col not in ['poem_id', 'Row_ID', 'Title_cleaned', 'Poem_line_cleaned', 'Poem_line_raw', 'Title_raw']:\n",
    "                    merged_value = merge_metadata_values(group[col], col)\n",
    "                    base_record[col] = merged_value\n",
    "            \n",
    "            all_records.append(base_record)\n",
    "            chunk_counter += 1\n",
    "        else:\n",
    "            # For poems with > 4 lines, use 4-line chunks with 2-line overlap\n",
    "            i = 0\n",
    "            while i < n:\n",
    "                end_idx = min(i + 4, n)\n",
    "                chunk = poem_lines[i:end_idx]\n",
    "                \n",
    "                # Get the corresponding group slice for metadata merging\n",
    "                start_row_idx = group.index[i]\n",
    "                end_row_idx = group.index[min(i + 4, n) - 1] if n > 0 else start_row_idx\n",
    "                chunk_group = group.loc[start_row_idx:end_row_idx]\n",
    "                \n",
    "                base_record = {\n",
    "                    'poem_id': poem_id,\n",
    "                    'chunk_id': chunk_counter,\n",
    "                    'Title_cleaned': title,\n",
    "                    'poem_lines_subset': format_chunk_content(chunk),\n",
    "                    'Poem_line_raw': \"\\n\".join([row['Poem_line_raw'] for _, row in chunk_group.iterrows()]) if 'Poem_line_raw' in chunk_group.columns else \"\",\n",
    "                    'Title_raw': merge_metadata_values(chunk_group['Title_raw'], 'Title_raw') if 'Title_raw' in chunk_group.columns else \"\"\n",
    "                }\n",
    "                \n",
    "                # Add ALL original columns\n",
    "                for col in group.columns:\n",
    "                    if col not in ['poem_id', 'Row_ID', 'Title_cleaned', 'Poem_line_cleaned', 'Poem_line_raw', 'Title_raw']:\n",
    "                        merged_value = merge_metadata_values(chunk_group[col], col)\n",
    "                        base_record[col] = merged_value\n",
    "                \n",
    "                all_records.append(base_record)\n",
    "                chunk_counter += 1\n",
    "                \n",
    "                # Move by 2 positions for 2-line overlap (4 - 2 = 2)\n",
    "                i += 2\n",
    "\n",
    "# Convert to DataFrame\n",
    "output_df = pd.DataFrame(all_records)\n",
    "\n",
    "# Build full poems for each poem_id by concatenating all original chunks\n",
    "poem_full_map = {}\n",
    "for poem_id, group in output_df.groupby('poem_id'):\n",
    "    # Join all chunks for this poem_id in order (sorted by chunk_id)\n",
    "    ordered_chunks = group.sort_values('chunk_id')['poem_lines_subset'].tolist()\n",
    "    full_poem = '\\n'.join(ordered_chunks)\n",
    "    poem_full_map[poem_id] = full_poem\n",
    "\n",
    "# Add the Full_poem column to the dataframe\n",
    "output_df['Full_poem'] = output_df['poem_id'].map(poem_full_map)\n",
    "\n",
    "# Reorder columns to have poem_id, chunk_id first, then Title_cleaned, poem_lines_subset, Full_poem, then others\n",
    "column_order = ['poem_id', 'chunk_id', 'Title_cleaned', 'poem_lines_subset', 'Full_poem']\n",
    "remaining_cols = [col for col in output_df.columns if col not in column_order]\n",
    "final_column_order = column_order + remaining_cols\n",
    "\n",
    "output_df = output_df[final_column_order]\n",
    "\n",
    "# Save to output CSV\n",
    "output_df.to_csv(\"07-chunked for semantic.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"✅ Done! {len(output_df)} rows generated.\")\n",
    "print(\"Columns in output:\", list(output_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1e436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
