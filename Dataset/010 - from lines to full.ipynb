{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a8f4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Poetry_App_Database.csv...\n",
      "❌ Error: Could not find Text column. Available: ['poem_id', 'entities', 'places', 'topics', 'religion', 'mood', 'category']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# --- 1. CONFIGURATION: YOUR DICTIONARIES ---\n",
    "topic_keywords = {\n",
    "    \"الحب والغزل\": [\"حب\", \"غرام\", \"عشق\", \"يهواك\", \"أحبك\", \"مغرم\", \"ولهان\", \"خل\", \"حبيبي\", \"الغلا\"],\n",
    "    \"الشوق والحنين\": [\"شوق\", \"حنين\", \"اشتياق\", \"مشتاق\", \"وينك\", \"غياب\", \"فقدتك\", \"يا طير\", \"ابطيت\"],\n",
    "    \"الفراق والهجر\": [\"فراق\", \"هجر\", \"وداع\", \"رحيل\", \"صد\", \"جفى\", \"فرقا\", \"توديع\", \"غيبتك\"],\n",
    "    \"الألم والحزن\": [\"حزن\", \"ألم\", \"جرح\", \"دمع\", \"بكى\", \"هم\", \"ضيق\", \"وجع\", \"مصاب\", \"عذاب\"],\n",
    "    \"الوفاء والصبر\": [\"وفاء\", \"صبر\", \"عهد\", \"وعد\", \"صابر\", \"مخلص\", \"وافي\", \"تضحية\"],\n",
    "    \"القيادة والزعماء\": [\"شيخ\", \"حاكم\", \"زايد\", \"محمد\", \"بوراشد\", \"بوخالد\", \"فزاع\", \"مجد\", \"شيوخ\", \"قائد\"],\n",
    "    \"المجد والعز\": [\"عز\", \"مجد\", \"فخر\", \"ناموس\", \"طناخة\", \"هيبة\", \"شموخ\", \"عالي\", \"قمة\"],\n",
    "    \"الشعر والإبداع\": [\"شعر\", \"قصيد\", \"بيوت\", \"قوافي\", \"أبيات\", \"لحن\", \"وزن\", \"معاني\", \"بوح\"],\n",
    "    \"حب الوطن\": [\"وطن\", \"إمارات\", \"دبي\", \"بلادي\", \"داري\", \"أرض\", \"علم\", \"اتحاد\"],\n",
    "    \"الطبيعة والخيل\": [\"خيل\", \"مهر\", \"صحراء\", \"بر\", \"مطر\", \"غيم\", \"بحر\", \"طير\", \"صيد\", \"مقناص\"],\n",
    "    \"الإيمان والدعاء\": [\"الله\", \"رب\", \"دعاء\", \"صلاة\", \"دين\", \"مؤمن\", \"حمد\", \"شكر\", \"استغفر\"],\n",
    "    \"التراث النبطي\": [\"هجن\", \"ذلول\", \"ناقة\", \"بدو\", \"شداد\", \"مركاض\", \"عزبة\", \"مقياظ\"],\n",
    "    \"الغيرة والعتاب\": [\"عتاب\", \"ليش\", \"ليه\", \"تلومني\", \"زعل\", \"خطا\", \"مسامح\", \"غلطان\"],\n",
    "    \"الذكريات\": [\"ذكرى\", \"أيام\", \"زمان\", \"ماضي\", \"تذكرت\", \"سنين\", \"طفولة\"],\n",
    "    \"الفخر والشجاعة\": [\"شجاعة\", \"إقدام\", \"سيف\", \"خوي\", \"رفيق\", \"نشاما\", \"كفو\"]\n",
    "}\n",
    "\n",
    "sentiment_keywords = {\n",
    "    \"حب\": [\"حب\", \"أحبك\", \"غلا\", \"عشق\"],\n",
    "    \"شوق\": [\"شوق\", \"وله\", \"مشتاق\", \"حنين\"],\n",
    "    \"حزن\": [\"حزن\", \"دمع\", \"بكاء\", \"ضيق\"],\n",
    "    \"فخر\": [\"فخر\", \"عز\", \"مجد\", \"كفو\"],\n",
    "    \"غضب\": [\"غضب\", \"زعل\", \"غيظ\", \"لوم\"],\n",
    "    \"خوف\": [\"خوف\", \"رعب\", \"وجل\"],\n",
    "    \"أمل\": [\"أمل\", \"تفاؤل\", \"بكره\", \"مستقبل\"],\n",
    "    \"ندم\": [\"ندم\", \"ليت\", \"حسافة\", \"توبة\"],\n",
    "    \"صبر\": [\"صبر\", \"تحمل\", \"جلد\"],\n",
    "    \"فرح\": [\"فرح\", \"سعادة\", \"سرور\", \"عيد\"],\n",
    "    \"حكمة\": [\"حكمة\", \"نصيحة\", \"تجارب\", \"عقل\"]\n",
    "}\n",
    "\n",
    "place_keywords = {\n",
    "    \"أراضي_إماراتية\": [\"دبي\", \"أبوظبي\", \"الشارقة\", \"عجمان\", \"أم القيوين\", \"رأس الخيمة\", \"الفجيرة\", \"العين\", \"جميرا\", \"زعبيل\", \"مرموم\", \"ليوا\"],\n",
    "    \"معالم_إماراتية\": [\"برج خليفة\", \"متحف\", \"قصر\", \"حصن\", \"مطار\"],\n",
    "    \"مواقع_دينية\": [\"مكة\", \"مدينة\", \"طيبة\", \"مسجد\", \"حرم\", \"كعبة\", \"عرفات\"],\n",
    "    \"مدن_وأماكن_عربية\": [\"رياض\", \"كويت\", \"بحرين\", \"عمان\", \"قطر\", \"قاهرة\", \"بغداد\", \"شام\", \"بيروت\"],\n",
    "    \"طبيعة_عامة\": [\"بحر\", \"جبل\", \"وادي\", \"نهر\", \"غابة\", \"صحراء\", \"روضة\"],\n",
    "    \"مواقع_نبطية\": [\"عد\", \"غدير\", \"رجم\", \"بيداء\", \"مراح\", \"مقيظ\"],\n",
    "    \"أماكن_مجردة\": [\"خيال\", \"حلم\", \"ذاكرة\", \"قلب\", \"عين\", \"بال\"],\n",
    "    \"قصر زعبيل\": [\"زعبيل\"]\n",
    "}\n",
    "\n",
    "# --- 2. HELPERS ---\n",
    "def find_col(df, candidates):\n",
    "    \"\"\" Finds column case-insensitively \"\"\"\n",
    "    cols = [c.lower() for c in df.columns]\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in cols:\n",
    "            # Return actual column name from df\n",
    "            return df.columns[cols.index(cand.lower())]\n",
    "    return None\n",
    "\n",
    "def parse_list(x):\n",
    "    try:\n",
    "        if pd.isna(x): return []\n",
    "        s = str(x).replace(\"'\", '\"')\n",
    "        if s.strip() == '[]' or s.strip() == '': return []\n",
    "        return json.loads(s)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def calculate_priority(text, existing_tags, keyword_map, max_count=3):\n",
    "    if not isinstance(existing_tags, list) or not existing_tags: \n",
    "        return []\n",
    "    if not text or pd.isna(text): \n",
    "        return existing_tags[:max_count]\n",
    "    \n",
    "    scores = Counter()\n",
    "    \n",
    "    # 1. Base points (AI Detection)\n",
    "    for tag in existing_tags:\n",
    "        scores[tag] += 2\n",
    "        \n",
    "    # 2. Density points (Text Scan)\n",
    "    text_clean = str(text).replace('\\n', ' ')\n",
    "    for category, keywords in keyword_map.items():\n",
    "        if category in existing_tags:\n",
    "            for word in keywords:\n",
    "                if word in text_clean:\n",
    "                    scores[category] += 1\n",
    "    \n",
    "    # 3. Sort & Slice\n",
    "    sorted_tags = [tag for tag, score in scores.most_common() if score > 0]\n",
    "    return sorted_tags[:max_count]\n",
    "\n",
    "# --- 3. MAIN LOGIC ---\n",
    "def run_final_polish(file_path):\n",
    "    print(f\"Reading {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Auto-detect Columns\n",
    "    text_col = find_col(df, ['full_text', 'full_poem_text', 'Poem_line_cleaned', 'poem_line_cleaned'])\n",
    "    topic_col = find_col(df, ['topics', 'mowadee'])\n",
    "    mood_col = find_col(df, ['mood', 'sentiments', 'sentiment'])\n",
    "    place_col = find_col(df, ['places', 'place_types', 'amaken'])\n",
    "    \n",
    "    if not text_col:\n",
    "        print(f\"❌ Error: Could not find Text column. Available: {list(df.columns)}\")\n",
    "        return\n",
    "\n",
    "    print(f\"✅ Found Columns: Text='{text_col}', Topics='{topic_col}', Mood='{mood_col}', Places='{place_col}'\")\n",
    "    print(\"Running Smart Priority Analysis...\")\n",
    "    \n",
    "    new_topics = []\n",
    "    new_moods = []\n",
    "    new_places = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        text = str(row[text_col])\n",
    "        \n",
    "        # Topics\n",
    "        curr_t = parse_list(row[topic_col]) if topic_col else []\n",
    "        new_topics.append(json.dumps(calculate_priority(text, curr_t, topic_keywords, 3), ensure_ascii=False))\n",
    "        \n",
    "        # Mood\n",
    "        curr_m = parse_list(row[mood_col]) if mood_col else []\n",
    "        new_moods.append(json.dumps(calculate_priority(text, curr_m, sentiment_keywords, 2), ensure_ascii=False))\n",
    "        \n",
    "        # Places\n",
    "        curr_p = parse_list(row[place_col]) if place_col else []\n",
    "        new_places.append(json.dumps(calculate_priority(text, curr_p, place_keywords, 2), ensure_ascii=False))\n",
    "\n",
    "    # Update DF\n",
    "    if topic_col: df[topic_col] = new_topics\n",
    "    if mood_col: df[mood_col] = new_moods\n",
    "    if place_col: df[place_col] = new_places\n",
    "    \n",
    "    output_file = 'Poetry_App_Database_FINAL.csv'\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✨ DONE. Database is now Optimized & Clean.\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Saved to: {output_file}\")\n",
    "\n",
    "# Run\n",
    "run_final_polish('Poetry_App_Database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbc50fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Poetry_App_Database.csv...\n",
      "Cleaning Columns based on Schema...\n",
      "\n",
      "==================================================\n",
      "✅ DONE. Columns Cleaned & Validated.\n",
      "==================================================\n",
      "Saved to: Poetry_App_Database_FINAL_CLEAN.csv\n",
      "\n",
      "Sample Row 1:\n",
      "poem_id                                                     1\n",
      "entities                                           [\"الحبيب\"]\n",
      "places                                                     []\n",
      "topics      [\"الحب والغزل\", \"الغيرة والعتاب\", \"التراث النب...\n",
      "religion                                                   []\n",
      "mood                                    [\"غضب\", \"فخر\", \"حزن\"]\n",
      "category                                            ['معاصر']\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "\n",
    "# --- 1. APPROVED SCHEMA (The Filter) ---\n",
    "allowed_topics = [\n",
    "    \"الحب والغزل\", \"الألم والحزن\", \"الوفاء والصبر\", \"الشوق والحنين\", \n",
    "    \"القيادة والزعماء\", \"المجد والعز\", \"الشعر والإبداع\", \"الطموح والنجاح\", \n",
    "    \"الفراق والهجر\", \"الجمال والعيون\", \"حب الوطن\", \"الغيرة والعتاب\", \n",
    "    \"الطبيعة والخيل\", \"الإيمان والدعاء\", \"التراث النبطي\", \"الزمن والقدر\", \n",
    "    \"الصداقة والكرم\", \"الفخر والشجاعة\", \"الإمارات ودبي\", \"الذكريات\"\n",
    "]\n",
    "\n",
    "allowed_places = [\n",
    "    \"أراضي_إماراتية\", \"أماكن_مجردة\", \"طبيعة_عامة\", \"مواقع_نبطية\", \n",
    "    \"مدن_وأماكن_عربية\", \"مواقع_اجتماعية\", \"مواقع_دينية\", \"معالم_إماراتية\", \n",
    "    \"قصر زعبيل\"\n",
    "]\n",
    "\n",
    "allowed_moods = [\n",
    "    \"حب\", \"فخر\", \"حزن\", \"شوق\", \"حكمة\", \"أمل\", \"غضب\", \n",
    "    \"صبر\", \"فرح\", \"حنين\", \"خوف\", \"ندم\", \"تأمل\", \"غيرة\"\n",
    "]\n",
    "\n",
    "# --- 2. SYNONYM MAPPING (Consolidating Duplicates) ---\n",
    "entity_map = {\n",
    "    \"حبيبتي\": \"الحبيب\",\n",
    "    \"الحبيبة\": \"الحبيب\",\n",
    "    \"المحبوبة\": \"الحبيب\",\n",
    "    \"المحبوب\": \"الحبيب\",\n",
    "    \"المتحدث\": \"الذات\",\n",
    "    \"الشاعر\": \"الذات\",\n",
    "    \"self\": \"الذات\"\n",
    "}\n",
    "\n",
    "# --- 3. PARSING LOGIC ---\n",
    "def safe_parse(cell):\n",
    "    \"\"\"Safely converts stringified lists/JSON into a Python list.\"\"\"\n",
    "    if pd.isna(cell) or str(cell).strip() == '': return []\n",
    "    try:\n",
    "        # Handle double encoding if present\n",
    "        s = str(cell).replace(\"'\", '\"')\n",
    "        if s.startswith('\"') and s.endswith('\"'): s = s[1:-1]\n",
    "        return json.loads(s)\n",
    "    except:\n",
    "        try: return ast.literal_eval(str(cell))\n",
    "        except: return []\n",
    "\n",
    "# --- 4. CLEANING FUNCTIONS ---\n",
    "def clean_entities(cell):\n",
    "    \"\"\"Parses entities, merges synonyms, removes duplicates.\"\"\"\n",
    "    items = safe_parse(cell)\n",
    "    cleaned = []\n",
    "    for x in items:\n",
    "        # Clean string\n",
    "        val = str(x).strip()\n",
    "        # Apply Map (e.g., 'حبيبتي' -> 'الحبيب')\n",
    "        val = entity_map.get(val, val)\n",
    "        cleaned.append(val)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    return list(dict.fromkeys(cleaned))\n",
    "\n",
    "def clean_generic(cell, allowed_list, max_items=4):\n",
    "    \"\"\"Filters against approved list and limits count.\"\"\"\n",
    "    items = safe_parse(cell)\n",
    "    cleaned = []\n",
    "    for x in items:\n",
    "        val = str(x).strip()\n",
    "        if val in allowed_list:\n",
    "            cleaned.append(val)\n",
    "    \n",
    "    # Remove duplicates & Limit\n",
    "    unique = list(dict.fromkeys(cleaned))\n",
    "    return unique[:max_items]\n",
    "\n",
    "# --- 5. MAIN EXECUTION ---\n",
    "def run_column_cleaner(file_path):\n",
    "    print(f\"Reading {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Verify columns exist\n",
    "    required_cols = ['entities', 'places', 'topics', 'mood']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            print(f\"❌ Error: Column '{col}' not found. Available: {list(df.columns)}\")\n",
    "            return\n",
    "\n",
    "    print(\"Cleaning Columns based on Schema...\")\n",
    "\n",
    "    # 1. Clean Entities (Merge synonyms)\n",
    "    df['entities'] = df['entities'].apply(clean_entities)\n",
    "\n",
    "    # 2. Clean Places (Filter & Limit)\n",
    "    df['places'] = df['places'].apply(lambda x: clean_generic(x, allowed_places, 2))\n",
    "\n",
    "    # 3. Clean Topics (Filter & Limit)\n",
    "    df['topics'] = df['topics'].apply(lambda x: clean_generic(x, allowed_topics, 3))\n",
    "\n",
    "    # 4. Clean Mood (Filter & Limit)\n",
    "    df['mood'] = df['mood'].apply(lambda x: clean_generic(x, allowed_moods, 3))\n",
    "\n",
    "    # Formatting as JSON strings for CSV storage\n",
    "    for col in ['entities', 'places', 'topics', 'mood', 'religion', 'category']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(lambda x: json.dumps(x, ensure_ascii=False) if isinstance(x, list) else x)\n",
    "\n",
    "    output_file = 'Poetry_App_Database_FINAL_CLEAN.csv'\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"✅ DONE. Columns Cleaned & Validated.\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Saved to: {output_file}\")\n",
    "    print(\"\\nSample Row 1:\")\n",
    "    print(df.iloc[0])\n",
    "\n",
    "# Run on your specific file\n",
    "run_column_cleaner('Poetry_App_Database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5d240b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define Arabic connecting letters (can connect to following letter)\n",
    "CONNECTING_LETTERS = set('بتثجحخسشصضطظعغفقكلمنهي')\n",
    "# Non-connecting letters (cannot connect to following letter)\n",
    "NON_CONNECTING_LETTERS = set('ادذرزow')\n",
    "\n",
    "def format_poem_for_visual_balance(poem_text):\n",
    "    lines = poem_text.strip().split('\\n')\n",
    "    \n",
    "    # Extract halves and store original structure\n",
    "    all_halves = []\n",
    "    pairs = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if '     ' in line:\n",
    "            left, right = line.split('     ', 1)\n",
    "            all_halves.append(left.strip())\n",
    "            all_halves.append(right.strip())\n",
    "            pairs.append([left.strip(), right.strip()])\n",
    "    \n",
    "    # Find max length per half position (left and right separately)\n",
    "    max_left = max(len(half) for half in all_halves[::2]) if all_halves[::2] else 0\n",
    "    max_right = max(len(half) for half in all_halves[1::2]) if all_halves[1::2] else 0\n",
    "    \n",
    "    # Balance each pair\n",
    "    balanced_pairs = []\n",
    "    for left_orig, right_orig in pairs:\n",
    "        left_balanced = distribute_tatweel_per_word(left_orig, max_left)\n",
    "        right_balanced = distribute_tatweel_per_word(right_orig, max_right)\n",
    "        balanced_pairs.append((left_balanced, right_balanced))\n",
    "    \n",
    "    # Reconstruct with 5 spaces\n",
    "    result_lines = []\n",
    "    for left, right in balanced_pairs:\n",
    "        result_lines.append(f\"{left}     {right}\")\n",
    "    \n",
    "    return '\\n'.join(result_lines)\n",
    "\n",
    "def distribute_tatweel_per_word(text, target_length):\n",
    "    current = text\n",
    "    words = re.findall(r'\\S+|\\s+', current)  # Split into words and spaces\n",
    "    \n",
    "    # First pass: identify all valid connection points in each word\n",
    "    connection_points = {}\n",
    "    for i, word in enumerate(words):\n",
    "        if not word.isspace():\n",
    "            points = []\n",
    "            for pos in range(len(word)-1):\n",
    "                char1 = word[pos]\n",
    "                char2 = word[pos+1]\n",
    "                # Check if char1 is a connecting letter and char2 is an Arabic letter\n",
    "                if char1 in CONNECTING_LETTERS and '\\u0627' <= char2 <= '\\u064a':\n",
    "                    points.append(pos+1)  # Position after char1\n",
    "            connection_points[i] = points\n",
    "    \n",
    "    while len(current) < target_length:\n",
    "        # Find shortest non-space word with available connection points\n",
    "        min_len = float('inf')\n",
    "        target_idx = -1\n",
    "        \n",
    "        for i, word in enumerate(words):\n",
    "            if not word.isspace() and i in connection_points and connection_points[i]:\n",
    "                if len(word) < min_len:\n",
    "                    min_len = len(word)\n",
    "                    target_idx = i\n",
    "        \n",
    "        if target_idx != -1 and connection_points[target_idx]:\n",
    "            # Add tatweel at the first available connection point\n",
    "            pos = connection_points[target_idx][0]\n",
    "            words[target_idx] = words[target_idx][:pos] + 'ـ' + words[target_idx][pos:]\n",
    "            # Update connection points (shift positions after insertion)\n",
    "            for p_idx in range(len(connection_points[target_idx])):\n",
    "                if connection_points[target_idx][p_idx] >= pos:\n",
    "                    connection_points[target_idx][p_idx] += 1\n",
    "            current = ''.join(words)\n",
    "        else:\n",
    "            # If no valid connection points, add at end of shortest word\n",
    "            min_len = float('inf')\n",
    "            end_idx = -1\n",
    "            for i, word in enumerate(words):\n",
    "                if not word.isspace() and len(word) < min_len:\n",
    "                    min_len = len(word)\n",
    "                    end_idx = i\n",
    "            if end_idx != -1:\n",
    "                words[end_idx] += 'ـ'\n",
    "                current = ''.join(words)\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    return current\n",
    "\n",
    "# Read CSV\n",
    "df = pd.read_csv('Poetry.csv')\n",
    "\n",
    "# Process only 'Poem_line_raw' column\n",
    "for idx, poem_text in enumerate(df['Poem_line_raw']):\n",
    "    if pd.notna(poem_text):  # Skip NaN values\n",
    "        formatted = format_poem_for_visual_balance(poem_text)\n",
    "        df.iloc[idx, df.columns.get_loc('Poem_line_raw')] = formatted\n",
    "\n",
    "# Save back to CSV\n",
    "df.to_csv('output_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad7bce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
