{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13871d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! 2170 rows generated.\n",
      "Columns in output: ['poem_id', 'chunk_id', 'Title_cleaned', 'poem_lines_subset', 'Summary_chunked', 'Title_raw', 'Poem_line_raw', 'قافية', 'البحر', 'وصل', 'حركة', 'نوع', 'شخص', 'sentiments', 'أماكن', 'أحداث', 'مواضيع', 'تصنيف']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import ast\n",
    "import json\n",
    "\n",
    "def merge_metadata_values(series_list, col_name):\n",
    "    \"\"\"Merge values from multiple rows, deduplicating appropriately\"\"\"\n",
    "    if col_name in ['poem_id', 'Row_ID', 'Title_cleaned', 'Poem_line_cleaned']:\n",
    "        return series_list.iloc[0] if not series_list.empty else \"\"\n",
    "    \n",
    "    # For JSON-like columns (person, sentiments, etc.)\n",
    "    if col_name in ['person', 'sentiments', 'أماكن', 'أحداث', 'مواضيع']:\n",
    "        all_values = []\n",
    "        for val in series_list:\n",
    "            if pd.notna(val) and val != \"\":\n",
    "                try:\n",
    "                    # Try parsing as JSON list/array\n",
    "                    parsed = ast.literal_eval(str(val)) if isinstance(val, str) and (val.startswith('[') or val.startswith('{')) else val\n",
    "                    if isinstance(parsed, list):\n",
    "                        all_values.extend(parsed)\n",
    "                    else:\n",
    "                        all_values.append(val)\n",
    "                except:\n",
    "                    all_values.append(val)\n",
    "        # Remove duplicates while preserving order\n",
    "        unique_values = []\n",
    "        seen = set()\n",
    "        for item in all_values:\n",
    "            item_str = json.dumps(item, sort_keys=True) if isinstance(item, dict) else str(item)\n",
    "            if item_str not in seen:\n",
    "                seen.add(item_str)\n",
    "                unique_values.append(item)\n",
    "        return unique_values\n",
    "    \n",
    "    # For simple text columns (summary, qafia, etc.), get unique values\n",
    "    else:\n",
    "        unique_values = list(set([str(v) for v in series_list if pd.notna(v) and v != \"\"]))\n",
    "        # Return the most common non-empty value, or join if multiple\n",
    "        if len(unique_values) == 1:\n",
    "            return unique_values[0]\n",
    "        else:\n",
    "            # For columns like summary, take the first one; for others, join unique values\n",
    "            if col_name == 'summary':\n",
    "                return series_list.iloc[0] if not series_list.empty else \"\"\n",
    "            else:\n",
    "                return \", \".join([v for v in unique_values if v])\n",
    "\n",
    "df = pd.read_csv(\"06-chunking exact search dataset for semantic.csv\")  # Replace with your actual filename\n",
    "\n",
    "required = ['poem_id', 'Row_ID', 'Title_cleaned', 'Poem_line_cleaned']\n",
    "for col in required:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Missing column: {col}\")\n",
    "\n",
    "df = df.sort_values(['poem_id', 'Row_ID']).reset_index(drop=True)\n",
    "df['Poem_line_cleaned'] = df['Poem_line_cleaned'].fillna('').astype(str)\n",
    "\n",
    "all_records = []\n",
    "chunk_counter = 1\n",
    "\n",
    "for poem_id, group in df.groupby('poem_id'):\n",
    "    title = group['Title_cleaned'].dropna().iloc[0] if not group['Title_cleaned'].dropna().empty else \"Untitled\"\n",
    "    poem_lines = [(row['Row_ID'], row['Poem_line_cleaned'], row) for _, row in group.iterrows()]\n",
    "    n = len(poem_lines)\n",
    "    \n",
    "    if n == 0:\n",
    "        # Handle empty poem case\n",
    "        base_record = {\n",
    "            'poem_id': poem_id,\n",
    "            'chunk_id': chunk_counter,\n",
    "            'Title_cleaned': title,\n",
    "            'poem_lines_subset': \"\",\n",
    "            'Summary_chunked': \"\",  # Map summary to Summary_chunked\n",
    "            'Title_raw': \"\",       # Will be taken from original data\n",
    "            'Poem_line_raw': \"\"    # Will be taken from original data\n",
    "        }\n",
    "        chunk_counter += 1\n",
    "        # Add all other columns with empty values\n",
    "        for col in group.columns:\n",
    "            if col not in ['poem_id', 'Row_ID', 'Title_cleaned', 'Poem_line_cleaned']:\n",
    "                base_record[col] = \"\"\n",
    "        all_records.append(base_record)\n",
    "    else:\n",
    "        full_raw = '\\n'.join(row['Poem_line_cleaned'] for _, _, row in poem_lines)\n",
    "        \n",
    "        def format_subset(lines_chunk):\n",
    "            if not lines_chunk:\n",
    "                return \"\"\n",
    "            formatted = [f'\"{rid}\":\"{line}\"' for rid, line, _ in lines_chunk]\n",
    "            if len(formatted) == 1:\n",
    "                return formatted[0]\n",
    "            return ',\\n'.join(formatted)\n",
    "\n",
    "        # Chunk with 4 lines and 2-line overlap\n",
    "        i = 0\n",
    "        while i < n:\n",
    "            end_idx = min(i + 4, n)\n",
    "            chunk = poem_lines[i:end_idx]\n",
    "            \n",
    "            # Get the corresponding group slice for metadata merging\n",
    "            start_row_idx = group.index[i]\n",
    "            end_row_idx = group.index[min(i + 4, n) - 1] if n > 0 else start_row_idx\n",
    "            chunk_group = group.loc[start_row_idx:end_row_idx]\n",
    "            \n",
    "            base_record = {\n",
    "                'poem_id': poem_id,\n",
    "                'chunk_id': chunk_counter,\n",
    "                'Title_cleaned': title,\n",
    "                'poem_lines_subset': format_subset(chunk),\n",
    "                'Summary_chunked': merge_metadata_values(chunk_group['summary'], 'summary') if 'summary' in chunk_group.columns else \"\",\n",
    "                'Title_raw': merge_metadata_values(chunk_group['Title_raw'], 'Title_raw') if 'Title_raw' in chunk_group.columns else \"\",\n",
    "                'Poem_line_raw': \"\\n\".join([row['Poem_line_raw'] for _, row in chunk_group.iterrows()]) if 'Poem_line_raw' in chunk_group.columns else \"\"\n",
    "            }\n",
    "            \n",
    "            # Add all other original columns\n",
    "            for col in group.columns:\n",
    "                if col not in ['poem_id', 'Row_ID', 'Title_cleaned', 'Poem_line_cleaned', 'summary', 'Title_raw', 'Poem_line_raw']:\n",
    "                    merged_value = merge_metadata_values(chunk_group[col], col)\n",
    "                    base_record[col] = merged_value\n",
    "            \n",
    "            all_records.append(base_record)\n",
    "            chunk_counter += 1\n",
    "            \n",
    "            # Move by 2 positions for 2-line overlap (4 - 2 = 2)\n",
    "            i += 2\n",
    "\n",
    "output_df = pd.DataFrame(all_records)\n",
    "\n",
    "# Reorder columns to match expected format\n",
    "expected_cols = ['poem_id', 'chunk_id', 'Title_cleaned', 'poem_lines_subset', 'Summary_chunked', 'Title_raw', 'Poem_line_raw']\n",
    "remaining_cols = [col for col in output_df.columns if col not in expected_cols]\n",
    "final_cols = expected_cols + remaining_cols\n",
    "\n",
    "output_df = output_df[final_cols]\n",
    "\n",
    "# Use a more robust CSV writing approach to handle special characters\n",
    "output_df.to_csv(\"07-chunked for semantic.csv\", \n",
    "                 index=False, \n",
    "                 encoding='utf-8-sig',\n",
    "                 quoting=1)  # Force quotes around all fields to prevent truncation\n",
    "\n",
    "print(f\"✅ Done! {len(output_df)} rows generated.\")\n",
    "print(\"Columns in output:\", list(output_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1e436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
